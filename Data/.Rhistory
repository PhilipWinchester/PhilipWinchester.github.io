StepToPoint <- Parameters + GradientVectorNormalised
print(GradientVectorNormalised)
LLLoop <- 0
# Adding GradientVectorNormalised until we have maxemised the LL
while(LL(Match_Data, Parameters=StepToPoint) > LL(Match_Data, Parameters=PresentPoint)){
PresentPoint <- StepToPoint
StepToPoint <- PresentPoint + GradientVectorNormalised
LLLoop <- LLLoop + 1
}
print(paste("LLLoop is "  ,toString(LLLoop)))
# If there has been less than one itteration in the while loop and the step size is smaller than the maximum, then we increase the step size
if(LLLoop < 2 & Step != Max | Step == Max & LLLoop == 0){
Mult <- Mult + 1
Step <- Mult*m
}
Parameters <- PresentPoint
}
Alpha <- Parameters[1:length(Teams)]
Beta <- Parameters[(length(Teams)+1):(length(Teams)*2)]
Gamma <- Parameters[length(Teams)*2+1]
Rho <- Parameters[length(Teams)*2+2]
Results <- data.frame(Teams, Alpha, Beta, Gamma, Rho)
return(Results)
}
# prints
Results <- Optimise(Match_Data) # If we run this, we will se that all params but gamma are in good shape. could have 2nd bit where we optemise thi
Optimise <- function(Match_Data, Max = 100, m = 10){
# Takes some match data and returns
Teams <- sort(unique(Match_Data$HomeTeam))
# Setting all Parameters equal to 1 at first
Parameters <- rep(1,2*length(Teams)+2)
# Setting gamma equal to 1.4 and rho equal to 0
Parameters[2*length(Teams)+1] <- 1.4
Parameters[2*length(Teams)+2] <- -0.05
# Making StartParameters
StartParameters <- rep(0,2*length(Teams)+2)
Mult <- 1
Step <- m
count <- 0
# NMod just finds the length betweent he vectores
while(NMod(Parameters-StartParameters,1) > 0| Mult < ((Max/m)+1)){
count <- count + 1
print(paste("count is "  ,toString(count)))
# Saving what wha have from the start
StartParameters <- Parameters
# Finding gradient
GradientVector <- GradientVectorFinder(Match_Data, Parameters)
# Normalising (Avergage of alhpas is 1), and adjusting the length
GradientVectorNormalised <- NormalisingTheGradientVector(GradientVector,Step)
print(paste("step is "  ,toString(Step)))
PresentPoint <- Parameters
StepToPoint <- Parameters + GradientVectorNormalised
print(GradientVectorNormalised)
LLLoop <- 0
# Adding GradientVectorNormalised until we have maxemised the LL
while(LL(Match_Data, Parameters=StepToPoint) > LL(Match_Data, Parameters=PresentPoint)){
PresentPoint <- StepToPoint
StepToPoint <- PresentPoint + GradientVectorNormalised
LLLoop <- LLLoop + 1
}
print(paste("LLLoop is "  ,toString(LLLoop)))
# If there has been less than one itteration in the while loop and the step size is smaller than the maximum, then we increase the step size
if(LLLoop < 2 & Step != Max | Step == Max & LLLoop == 0){
Mult <- Mult + 1
Step <- Mult*m
}
Parameters <- PresentPoint
}
Alpha <- Parameters[1:length(Teams)]
Beta <- Parameters[(length(Teams)+1):(length(Teams)*2)]
Gamma <- Parameters[length(Teams)*2+1]
Rho <- Parameters[length(Teams)*2+2]
Results <- data.frame(Teams, Alpha, Beta, Gamma, Rho)
return(Results)
}
# prints
Results <- Optimise(Match_Data) # If we run this, we will se that all params but gamma are in good shape. could have 2nd bit where we optemise thi
Optimise <- function(Match_Data, Max = 100, m = 10){
# Takes some match data and returns
Teams <- sort(unique(Match_Data$HomeTeam))
# Setting all Parameters equal to 1 at first
Parameters <- rep(1,2*length(Teams)+2)
# Setting gamma equal to 1.4 and rho equal to 0
Parameters[2*length(Teams)+1] <- 1.4
Parameters[2*length(Teams)+2] <- -0.05
# Making StartParameters
StartParameters <- rep(0,2*length(Teams)+2)
Mult <- 1
Step <- m
count <- 0
# NMod just finds the length betweent he vectores
while(NMod(Parameters-StartParameters,1) > 0| Mult < ((Max/m)+1)){
count <- count + 1
print(paste("count is "  ,toString(count)))
# Saving what wha have from the start
StartParameters <- Parameters
# Finding gradient
GradientVector <- GradientVectorFinder(Match_Data, Parameters)
# Normalising (Avergage of alhpas is 1), and adjusting the length
GradientVectorNormalised <- NormalisingTheGradientVector(GradientVector,Step)
print(paste("step is "  ,toString(Step)))
PresentPoint <- Parameters
StepToPoint <- Parameters + GradientVectorNormalised
print(GradientVectorNormalised)
LLLoop <- 0
# Adding GradientVectorNormalised until we have maxemised the LL
while(LL(Match_Data, Parameters=StepToPoint) > LL(Match_Data, Parameters=PresentPoint)){
PresentPoint <- StepToPoint
StepToPoint <- PresentPoint + GradientVectorNormalised
LLLoop <- LLLoop + 1
}
print(paste("LLLoop is "  ,toString(LLLoop)))
# If there has only been one itteration, we increase the step size
if(LLLoop < 2){
Mult <- Mult + 1
Step <- Mult*m
}
Parameters <- PresentPoint
}
Alpha <- Parameters[1:length(Teams)]
Beta <- Parameters[(length(Teams)+1):(length(Teams)*2)]
Gamma <- Parameters[length(Teams)*2+1]
Rho <- Parameters[length(Teams)*2+2]
Results <- data.frame(Teams, Alpha, Beta, Gamma, Rho)
return(Results)
}
# prints
Results <- Optimise(Match_Data) # If we run this, we will se that all params but gamma are in good shape. could have 2nd bit where we optemise thi
Optimise <- function(Match_Data, Max = 100, m = 10){
# Takes some match data and returns
Teams <- sort(unique(Match_Data$HomeTeam))
# Setting all Parameters equal to 1 at first
Parameters <- rep(1,2*length(Teams)+2)
# Setting gamma equal to 1.4 and rho equal to 0
Parameters[2*length(Teams)+1] <- 1.4
Parameters[2*length(Teams)+2] <- -0.05
# Making StartParameters
StartParameters <- rep(0,2*length(Teams)+2)
Mult <- 1
Step <- m
count <- 0
# NMod just finds the length betweent he vectores
while(Step < Max){
count <- count + 1
print(paste("count is "  ,toString(count)))
# Saving what wha have from the start
StartParameters <- Parameters
# Finding gradient
GradientVector <- GradientVectorFinder(Match_Data, Parameters)
# Normalising (Avergage of alhpas is 1), and adjusting the length
GradientVectorNormalised <- NormalisingTheGradientVector(GradientVector,Step)
print(paste("step is "  ,toString(Step)))
PresentPoint <- Parameters
StepToPoint <- Parameters + GradientVectorNormalised
print(GradientVectorNormalised)
LLLoop <- 0
# Adding GradientVectorNormalised until we have maxemised the LL
while(LL(Match_Data, Parameters=StepToPoint) > LL(Match_Data, Parameters=PresentPoint)){
PresentPoint <- StepToPoint
StepToPoint <- PresentPoint + GradientVectorNormalised
LLLoop <- LLLoop + 1
}
print(paste("LLLoop is "  ,toString(LLLoop)))
# If there has only been one itteration, we increase the step size
if(LLLoop < 2){
Mult <- Mult + 1
Step <- Mult*m
}
Parameters <- PresentPoint
}
Alpha <- Parameters[1:length(Teams)]
Beta <- Parameters[(length(Teams)+1):(length(Teams)*2)]
Gamma <- Parameters[length(Teams)*2+1]
Rho <- Parameters[length(Teams)*2+2]
Results <- data.frame(Teams, Alpha, Beta, Gamma, Rho)
return(Results)
}
# prints
Results <- Optimise(Match_Data) # If we run this, we will se that all params but gamma are in good shape. could have 2nd bit where we optemise thi
Optimise <- function(Match_Data, Max = 100, m = 10){
# Takes some match data and returns
Teams <- sort(unique(Match_Data$HomeTeam))
# Setting all Parameters equal to 1 at first
Parameters <- rep(1,2*length(Teams)+2)
# Setting gamma equal to 1.4 and rho equal to 0
Parameters[2*length(Teams)+1] <- 1.4
Parameters[2*length(Teams)+2] <- -0.05
# Making StartParameters
StartParameters <- rep(0,2*length(Teams)+2)
Mult <- 1
Step <- m
count <- 0
# NMod just finds the length betweent he vectores
while(Step <= Max){
count <- count + 1
print(paste("count is "  ,toString(count)))
# Saving what wha have from the start
StartParameters <- Parameters
# Finding gradient
GradientVector <- GradientVectorFinder(Match_Data, Parameters)
# Normalising (Avergage of alhpas is 1), and adjusting the length
GradientVectorNormalised <- NormalisingTheGradientVector(GradientVector,Step)
print(paste("step is "  ,toString(Step)))
PresentPoint <- Parameters
StepToPoint <- Parameters + GradientVectorNormalised
print(GradientVectorNormalised)
LLLoop <- 0
# Adding GradientVectorNormalised until we have maxemised the LL
while(LL(Match_Data, Parameters=StepToPoint) > LL(Match_Data, Parameters=PresentPoint)){
PresentPoint <- StepToPoint
StepToPoint <- PresentPoint + GradientVectorNormalised
LLLoop <- LLLoop + 1
}
print(paste("LLLoop is "  ,toString(LLLoop)))
# If there has only been one itteration, we increase the step size
if(LLLoop < 2){
Mult <- Mult + 1
Step <- Mult*m
}
Parameters <- PresentPoint
}
Alpha <- Parameters[1:length(Teams)]
Beta <- Parameters[(length(Teams)+1):(length(Teams)*2)]
Gamma <- Parameters[length(Teams)*2+1]
Rho <- Parameters[length(Teams)*2+2]
Results <- data.frame(Teams, Alpha, Beta, Gamma, Rho)
return(Results)
}
# prints
Results <- Optimise(Match_Data) # If we run this, we will se that all params but gamma are in good shape. could have 2nd bit where we optemise thi
# Plotting curve that shows different epsilons
curve(exp(-0.001*x), xlim = c(0, 1000), ylim = c(0,1), n = 1001, col = "green", xlab = "t", ylab = "φ(t)")
curve(exp(-0.002*x), xlim = c(0, 1000), n = 1001, col = "orange", add = TRUE)
curve(exp(-0.003*x), xlim = c(0, 1000), n = 1001, col = "red", add = TRUE)
legend("topright", c("ε = 0.001", "ε = 0.002","ε = 0.003"), col = c("green", "orange", "red"), pch=16)
Optimise <- function(Match_Data, Max = 100, m = 10){
# Takes some match data and returns
Teams <- sort(unique(Match_Data$HomeTeam))
# Setting all Parameters equal to 1 at first
Parameters <- rep(1,2*length(Teams)+2)
# Setting gamma equal to 1.4 and rho equal to 0
Parameters[2*length(Teams)+1] <- 1.3
Parameters[2*length(Teams)+2] <- -0.05
Mult <- 1
Step <- m
count <- 0
# NMod just finds the length betweent he vectores
while(Step <= Max){
count <- count + 1
print(paste("count is "  ,toString(count)))
# Finding gradient
GradientVector <- GradientVectorFinder(Match_Data, Parameters)
# Normalising (Avergage of alhpas is 1), and adjusting the length
GradientVectorNormalised <- NormalisingTheGradientVector(GradientVector,Step)
print(paste("step is "  ,toString(Step)))
PresentPoint <- Parameters
StepToPoint <- Parameters + GradientVectorNormalised
LLLoop <- 0
# Adding GradientVectorNormalised until we have maxemised the LL
while(LL(Match_Data, Parameters=StepToPoint) > LL(Match_Data, Parameters=PresentPoint)){
PresentPoint <- StepToPoint
StepToPoint <- PresentPoint + GradientVectorNormalised
LLLoop <- LLLoop + 1
}
print(paste("LLLoop is "  ,toString(LLLoop)))
# If there has only been one itteration, we increase the step size
if(LLLoop < 2){
Mult <- Mult + 1
Step <- Mult*m
}
Parameters <- PresentPoint
}
Alpha <- Parameters[1:length(Teams)]
Beta <- Parameters[(length(Teams)+1):(length(Teams)*2)]
Gamma <- Parameters[length(Teams)*2+1]
Rho <- Parameters[length(Teams)*2+2]
Results <- data.frame(Teams, Alpha, Beta, Gamma, Rho)
return(Results)
}
# prints
Results <- Optimise(Match_Data) # If we run this, we will se that all params but gamma are in good shape. could have 2nd bit where we optemise thi
View(Results)
NMod(c(1,2),100)
NMOD((1,2)/NMod(c(1,2),100),1)
NMod((1,2)/NMod(c(1,2),100),1)
NMod(c(1,2)/NMod(c(1,2),100),1)
NMod(c(1,2,4)/NMod(c(1,2,4),100),1)
NMod(c(1,2,4)/NMod(c(1,2,4),50),1)
NMod<- function(Vector,n=1){
# Takes vector and returns n*mod
NMod <- 0
for(i in 1:length(Vector)){
NMod <- NMod + Vector[i]^2
}
return((NMod)^(0.5)*n)
}
Optimise <- function(Match_Data, Max = 100, m = 10){
# Takes some match data and returns
Teams <- sort(unique(Match_Data$HomeTeam))
# Setting all Parameters equal to 1 at first
Parameters <- rep(1,2*length(Teams)+2)
# Setting gamma equal to 1.4 and rho equal to 0
Parameters[2*length(Teams)+1] <- 1.3
Parameters[2*length(Teams)+2] <- -0.05
# Making StartParameters
StartParameters <- rep(0,2*length(Teams)+2)
Mult <- 1
Step <- m
count <- 0
# Doing itertaitons until we have added just one of the smallets gradient vecor we weant to add
while(NMod(Parameters-StartParameters) != 1/Max){
count <- count + 1
print(paste("count is "  ,toString(count)))
# Saving what wha have from the start
StartParameters <- Parameters
# Finding gradient
GradientVector <- GradientVectorFinder(Match_Data, Parameters)
# Normalising (Avergage of alhpas is 1), and adjusting the length
GradientVectorNormalised <- NormalisingTheGradientVector(GradientVector,Step)
print(paste("step is "  ,toString(Step)))
PresentPoint <- Parameters
StepToPoint <- Parameters + GradientVectorNormalised
LLLoop <- 0
# Adding GradientVectorNormalised until we have maxemised the LL
while(LL(Match_Data, Parameters=StepToPoint) > LL(Match_Data, Parameters=PresentPoint)){
PresentPoint <- StepToPoint
StepToPoint <- PresentPoint + GradientVectorNormalised
LLLoop <- LLLoop + 1
}
print(paste("LLLoop is "  ,toString(LLLoop)))
# If there has only been one itteration, we increase the step size
if(LLLoop < 2){
Mult <- Mult + 1
Step <- Mult*m
}
Parameters <- PresentPoint
}
Alpha <- Parameters[1:length(Teams)]
Beta <- Parameters[(length(Teams)+1):(length(Teams)*2)]
Gamma <- Parameters[length(Teams)*2+1]
Rho <- Parameters[length(Teams)*2+2]
Results <- data.frame(Teams, Alpha, Beta, Gamma, Rho)
return(Results)
}
# prints
Results <- Optimise(Match_Data) # If we run this, we will se that all params but gamma are in good shape. could have 2nd bit where we optemise thi
2 != 3
2 != 2
Optimise <- function(Match_Data, Max = 100, m = 10){
# Takes some match data and returns
Teams <- sort(unique(Match_Data$HomeTeam))
# Setting all Parameters equal to 1 at first
Parameters <- rep(1,2*length(Teams)+2)
# Setting gamma equal to 1.4 and rho equal to 0
Parameters[2*length(Teams)+1] <- 1.3
Parameters[2*length(Teams)+2] <- -0.05
# Making StartParameters
StartParameters <- rep(0,2*length(Teams)+2)
Mult <- 1
Step <- m
count <- 0
# Doing itertaitons until we have added just one of the smallets gradient vecor we weant to add
while(NMod(Parameters-StartParameters) != 1/Max){
print(NMod(Parameters-StartParameters))
count <- count + 1
print(paste("count is "  ,toString(count)))
# Saving what wha have from the start
StartParameters <- Parameters
# Finding gradient
GradientVector <- GradientVectorFinder(Match_Data, Parameters)
# Normalising (Avergage of alhpas is 1), and adjusting the length
GradientVectorNormalised <- NormalisingTheGradientVector(GradientVector,Step)
print(paste("step is "  ,toString(Step)))
PresentPoint <- Parameters
StepToPoint <- Parameters + GradientVectorNormalised
LLLoop <- 0
# Adding GradientVectorNormalised until we have maxemised the LL
while(LL(Match_Data, Parameters=StepToPoint) > LL(Match_Data, Parameters=PresentPoint)){
PresentPoint <- StepToPoint
StepToPoint <- PresentPoint + GradientVectorNormalised
LLLoop <- LLLoop + 1
}
print(paste("LLLoop is "  ,toString(LLLoop)))
# If there has only been one itteration, we increase the step size
if(LLLoop < 2){
Mult <- Mult + 1
Step <- Mult*m
}
Parameters <- PresentPoint
}
Alpha <- Parameters[1:length(Teams)]
Beta <- Parameters[(length(Teams)+1):(length(Teams)*2)]
Gamma <- Parameters[length(Teams)*2+1]
Rho <- Parameters[length(Teams)*2+2]
Results <- data.frame(Teams, Alpha, Beta, Gamma, Rho)
return(Results)
}
# prints
Results <- Optimise(Match_Data) # If we run this, we will se that all params but gamma are in good shape. could have 2nd bit where we optemise thi
2 <= 2
1 <= 2
3 <= 2
# prints
Results <- Optimise(Match_Data, Max = 200) # If we run this, we will se that all params but gamma are in good shape. could have 2nd bit where we optemise thi
Optimise <- function(Match_Data, Max = 100, m = 10){
# Takes some match data and returns
Teams <- sort(unique(Match_Data$HomeTeam))
# Setting all Parameters equal to 1 at first
Parameters <- rep(1,2*length(Teams)+2)
# Setting gamma equal to 1.4 and rho equal to 0
Parameters[2*length(Teams)+1] <- 1.3
Parameters[2*length(Teams)+2] <- -0.05
Mult <- 1
Step <- m
count <- 0
# Doing itertaitons until we have added just one of the smallets gradient vecor we weant to add
while(Step <= Max){
count <- count + 1
print(paste("count is "  ,toString(count)))
# Finding gradient
GradientVector <- GradientVectorFinder(Match_Data, Parameters)
# Normalising (Avergage of alhpas is 1), and adjusting the length
GradientVectorNormalised <- NormalisingTheGradientVector(GradientVector,Step)
print(paste("step is "  ,toString(Step)))
PresentPoint <- Parameters
StepToPoint <- Parameters + GradientVectorNormalised
LLLoop <- 0
# Adding GradientVectorNormalised until we have maxemised the LL
while(LL(Match_Data, Parameters=StepToPoint) > LL(Match_Data, Parameters=PresentPoint)){
PresentPoint <- StepToPoint
StepToPoint <- PresentPoint + GradientVectorNormalised
LLLoop <- LLLoop + 1
}
print(paste("LLLoop is "  ,toString(LLLoop)))
# If there has only been one itteration (or zero), we increase the step size
if(LLLoop < 2){
Mult <- Mult + 1
Step <- Mult*m
}
Parameters <- PresentPoint
}
Alpha <- Parameters[1:length(Teams)]
Beta <- Parameters[(length(Teams)+1):(length(Teams)*2)]
Gamma <- Parameters[length(Teams)*2+1]
Rho <- Parameters[length(Teams)*2+2]
Results <- data.frame(Teams, Alpha, Beta, Gamma, Rho)
return(Results)
}
# prints
Results <- Optimise(Match_Data, Max = 200) # If we run this, we will se that all params but gamma are in good shape. could have 2nd bit where we optemise thi
View(Results)
round_df <- function(x, digits) {
# round all numeric variables
# x: data frame
# digits: number of digits to round
numeric_columns <- sapply(x, mode) == 'numeric'
x[numeric_columns] <-  round(x[numeric_columns], digits)
x
}
Results <- round_df(Results,2)
Results[2]
ncol(Results)
round_df <- function(x, digits) {
# round all numeric variables
# x: data frame
# digits: number of digits to round
for(i in 2:ncol(x))
x[i] <-  round(x[i], digits)
return(x)
}
round_df <- function(x, digits) {
# round all numeric variables
# x: data frame
# digits: number of digits to round
for(i in 2:ncol(x))
x[i] <-  round(x[i], digits)
return(x)
}
Results <- round_df(Results,2)
View(Results)
round_df <- function(x, digits) {
# round all numeric variables
# x: data frame
# digits: number of digits to round
for(i in 2:ncol(x)){
x[i] <-  round(x[i], digits)
}
return(x)
}
# prints
Results <- Optimise(Match_Data, Max = 200) # If we run this, we will se that all params but gamma are in good shape. could have 2nd bit where we optemise thi
Results <- round_df(Results,2)
View(Results)
View(Results)
# Read in dataframe with results from last three EPL and Championship results
Match_Data_Original <- read.csv("Match Data.csv", header = TRUE, stringsAsFactors=FALSE)
# Todays date
Today <- as.Date(Sys.Date())
# Extracting columns we are interested in, and adding the time column
Match_Data <- Match_Data_Original[c("HomeTeam", "AwayTeam", "FTHG", "FTAG", "Date", "Div")] %>% mutate(t = as.integer(Today-as.Date(gsub("/", ".", Date), "%d.%m.%Y"))) %>% filter(Div == "E0")
View(Match_Data)
# Read in dataframe with results from last three EPL and Championship results
Match_Data_Original <- read.csv("Match Data.csv", header = TRUE, stringsAsFactors=FALSE)
# Todays date
Today <- as.Date(Sys.Date())
# Extracting columns we are interested in, and adding the time column
Match_Data <- Match_Data_Original[c("HomeTeam", "AwayTeam", "FTHG", "FTAG", "Date", "Div")] %>% mutate(t = as.integer(Today-as.Date(gsub("/", ".", Date), "%d.%m.%Y"))) %>% filter(Div == "E0")
View(Match_Data)
# prints
Results <- Optimise(Match_Data, Max = 200) # If we run this, we will se that all params but gamma are in good shape. could have 2nd bit where we optemise thi
Results <- round_df(Results,2)
View(Results)
